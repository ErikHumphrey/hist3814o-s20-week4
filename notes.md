# Notes â€” Week 4

## Excel & R

* I have an intermediate understanding of Excel, but it's a good idea to do this brief exercise on the basics.
* I've heard a lot jokes about pivot tables coming up a lot on job interviews and people not having much experience with them. They're really not all that complicated, I think.
* To get the sum of female children from 1 to 4 years of age, I put the formula `=SUM(P15500:P15503)` in cell R15500.

![An Excel worksheet with a chart generated](https://i.imgur.com/bTqvNjl.jpg "An Excel worksheet with a chart generated")  
*An Excel worksheet with a chart generated*

### Basic counting and plotting in R

* I noticed that the plot preview generated by R resizes dynamically to fit the window, hiding labels and such if necessary. This makes sense given the ability to zoom with the network graphs in Week 3, but it's nice to know that it's infinitely scalable. Although they're still flexible, some other statistical packages I've used, like Minitab, felt more like they were generating raster graphics that were teh same as if they were exported as an image. In a sense, both are still "what you see is what you get" (WYSIWYG), though.
* One drawback of R Studio is that you can't drag or copy resulting plots, tables, etc. directly into documents and spreadsheets like you can with Minitab.

![Number of articles per city bar plot](https://i.imgur.com/NpwhLmJ.jpg "Number of articles per city bar plot")  
*Number of articles per city bar plot*

* I tried all the visualizations under "Preface" and "Basic Histogram", and made a note to try out the rest later. Line graphs and scatter plots drew the most appeal to me, and would definitely be the most useful visualization for certain types of data.

![The visualization outputted by hist(unicorns$birthweight, breaks = c(0,1,2,3,4,5,6,7))](https://i.imgur.com/nqy9iTN.jpg "The visualization outputted by hist(unicorns$birthweight, breaks = c(0,1,2,3,4,5,6,7))")  
*The visualization outputted by `hist(unicorns$birthweight, breaks = c(0,1,2,3,4,5,6,7))`*

## Voyant

* I had never heard of this tool before.

### Introduction

* The results from Voyant loaded way more quickly than the results from Antconc! 
* The results are sharable! That's really useful for showing your work and sharing your findings without making people go through all the same steps you did.
* Here's the URL to my corpus: https://voyant-tools.org/?corpus=7c3404c9e6ab242ebf4c35831ef89267
  * The ID on its own: `7c3404c9e6ab242ebf4c35831ef89267`
* Once you've pressed "Reveal", it looks like there's little to customize that's saved. There are lots of different ways of looking through the corpus, the previews, and visualizations, but it appears all the customization needs to be done on the home page before hitting "Reveal".

![The Voyant results after pressing "Reveal"](https://i.imgur.com/C8sFlHU.jpg "The Voyant results after pressing \"Reveal\"")  
*The Voyant results after pressing "Reveal"*

### Explore the newspaper corpus

* Though I could guess the etymology from the "corp" part, I sort of forgot what "corpus" meant. I looked it up: 
  > corpus: a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject. *(Source: Google, `define corpus`)*
* Clicking on a term made the "trends" section on the right show the relative frequency of the term across all the documents in the corpus.
  * Double-clicking the term also opened up a small window with a miniature distribution, collocates, correlations, and phrases in which the term appeared. This also had a slider to adjust the number of items.
  * Checking the box beside another term allowed for comparison of multiple terms on the same trend chart by overlaying their individual charts.

![Key words in the context of "mr" in Voyant](https://i.imgur.com/6rfpnqn.jpg "Key words in the context of \"mr\" in Voyant")  
*Key words in the context of "mr" in Voyant*

* Being able to easily export the output of any of the many tools in a variety of ways is a welcome feature.
  * Here's the HTML snippet of the "Links" panel (a collocate graph) to try out later. I would imagine it would look exactly how it looks in Voyant.
  
    ```html
    <iframe style='width: 637px; height: 529px;' src='https://voyant-tools.org/tool/CollocatesGraph/?query=new&query=country&query=mr&mode=corpus&corpus=7c3404c9e6ab242ebf4c35831ef89267'></iframe>
    ```

* The stop list worked as expected.

### Building my own corpus

* In the Chronicling America database, I searched for "virtual" and picked out an article that had sharp, dark, clear text that I felt would be easy to use OCR on.
* Part way through my exploration, Chronicling America encountered an HTTP 502 Bad Gateway error every time I submitted any sort of request, so I had to stop working on it for a bit.

### Going further

* It's nice to have an offline virtual server version of Voyant. I don't have a use for installing it right now, but I definitely will in the future.
* Previously, I hadn't considered "working with sensitive materials" as a reason for running a local server. That makes sense now!
* I think it's great that Voyant is free and open-source software (FOSS)! I'd love to check out how it works some time.

## AntConc

* I had also never heard of this one before.
* AntConc was even easier to install than OpenRefine from Week 3; the Windows download was simply an executable (EXE). I chose to use the 64-bit version, and it ran fine without having to install any dependencies I don't yet have.
* I followed the tutorial with the given corpus with no issues.
  * Loading the corpus folder revealed that it had a total of 3080 TXT files.
  * I made a simple search for "the". During this time, the program was predictably unresponsive, but after allowing it a couple minutes to process all the files and several more minutes for it to count all the Concordance Hits (or process something else about all the files), it returned that "the" appeared at 488,253 times across all the files (case-insensitive, because I left "Case" unchecked). 
  
  ![Search results for "the"](https://i.imgur.com/HnlQMrt.jpg "Search results for \"the\"")  
  *Search rsults for "the"*

  I noticed that nouns mostly apepared after "the" (not pictured), with a variety of types of words appearing before "the".

  * I did it again for "a". Concordance Hits: 225,822. That's less than half the number of "the" hits. 
  
  Though "the" and "a" are different kinds of articles, the words appearing near them weren't too different. It surprised me that there were more instances of "the" than "a". I guess the writing style here demands more use of definite articles (before a noun that is known to the reader), rather than indefinite articles (for general objects or objects of unknown identity).
  * I did it again for "shot". Concordance Hits: 546.
  * The sort feature is handy!
  * I'm guessing a search for `wh*` would allow anything after "wh" in the word, like "while", "when", and "where", whereas `wh?` would only show words with three characters, like "why" or "who", but not "whom". This would also match the descriptions in the Wildcard Settings interface.
  * It's also worth noting that `wh?` requires a character, whereas `wh+` would also find just "wh". `wh*` can find "wh".
  * I was right. For posterity, `wh*` had 346,960 Concordance Hits, while `wh?` had 331,460 Concordance Hits. Surprisingly close!
  * I saved the sorted outputs "wh-asterisk-sorted.txt" and "wh-questionmark-sorted.txt".
  * The "or" operator (`|`), collocates, word lists, and keyword lists also seem like extremely useful tools for a variety of comparisons.

  ![The Word List tool](https://i.imgur.com/eMXLntT.jpg "The Word List tool")  
  *The Word List tool*
* I was pleasantly surprised to see that AntConc allowed for wildcard settings to be customized.

### Going further

* I've made a note to complete the Jupyter notebook later. For now, I gleaned it to get the gist of it. The graphical interface of AntConc is simple to use, but limited compared to the programmatic approach of using Python libraries. There are pros and cons to both.
  * Incidentally, Jupyter has always been a pretty good way of organizing a walkthrough, and the interactivity it allows is pretty nice. The last time I used it was for generating some sort of deep learning / machine learning model (or neural network?) that allowed me to feed in a document of facts and then input questions, after which it would return the most likely answer to the question given the facts. Microsoft Azure's [QnA Maker](https://azure.microsoft.com/en-us/services/cognitive-services/qna-maker/) API has a similar idea with a more user-friendly approach. But I digress.

## Topic models

### Topic modelling with the Topic Modeling Tool ("the TMTool")

* Having an "input" and "output" folder really seems to be common with machine learning and neural network-type tools, like [waifu2x](https://github.com/nagadomi/waifu2x). 
* MALLET stands for **MA**chine **L**earning for **L**anguag**E** **T**oolkit. It's open-source software.
* I was able to install the TMTool [using the fork of a working fork of the TMTool repository](https://github.com/shawngraham/topic-modeling-tool-1), found by checking the Hypothesis annotation on the instructions page.
  * It's a good thing I installed Java earlier, as it seems this application might depend on it.
* The program outputted "This could take minutes ***or days*** depending on settings and corpus size." I believe it.
  * With 10 topics and the given output, it took approximately 2 minutes to complete on my machine.

### But what does it mean?

* I succeeded in following Wallace's tutorial to generate a pie chart of each topic's contribution.

  ![Contribution pie chart](https://i.imgur.com/ijMbH1o.jpg "Contribution pie chart")  
  *Contribution pie chart*

  * I also experimented with pivot tables and line charts, but I don't feel they were very applicable to any of the generated data sets. 

* Though the tool is quite impressive and feels familiar, I don't think I would personally use this tool much. Maybe I'll try it again with different data in the future, but the tool's output requires quite a bit of manual analysis after it outputs the results. Though if you can pull off the analysis, the results certainly prove useful.

### A walkthrough to topic models in R

* Ah, I was hoping the course would explore the usage of projects in R, and not just indivudal files. This should be useful.
* To get the top 10 terms per topic rather than the top 5 terms, I could change `top5termsPerTopic <- terms(topicModel, 5)` to be `top5termsPerTopic <- terms(topicModel, 10)` (then replacing the variable name on the next line).
* I had to run `install.packages('pals')` to get `require(pals)` to work.
* R had a few issues setting some variables due it not being able to find certain functions like `posterior()` or `LDA()`. 
* I needed to install the `topicmodels` package and include the library, as the require statement wasn't enough. According to the official R documentation, it looks like the `BayesFactor` or `HMM` ("Hidden Markov Model") modules also share similar functions. (Hmm...)
* After R went through sampling 500 iterations, it outputted the pretty visualization.

  ![Topic proportions per decade bar plot, generated with R](https://i.imgur.com/eI6CSvG.jpg "Topic proportions per decade bar plot, generated with R")  
  *Topic proportions per decade bar plot, generated with R*

* Understandably, changing the top X terms per topic had no effect on the plot generated.

## Bonus

### Image analysis

* The images on this page remind me of the cryptic images used in [wargames / "Capture The Flag" cyber-security challenges](https://en.wikipedia.org/wiki/Wargame_(hacking)). I've done a few iterations of a beginner-level event at Carleton University, [Hack All The Things](https://h4tt.ca/).
* The *Popular Science Monthly* magazine covers are from around the First World War and the Second World War. The art style is certainly reminiscent of propaganda poster art around the time.
* I deleted the files mentioned and corrected some file names. I also made many file names more descriptive of the image, to match the others.
* Imj proved to be yet another extremely useful tool, which I'll be sure to use again in the future.

![A custom 'barcode' visualization](https://i.imgur.com/n2dwb6S.png "A custom 'barcode' visualization")  
*A custom "barcode" visualization*

![A custom 'montage' visualization](https://i.imgur.com/wUmIJJ4.png "A custom 'montage' visualization")  
*A custom "montage" visualization*

![A custom 'plot' visualization (x-axis: saturation, y-axis: brightness)](https://i.imgur.com/fwshffQ.png "A custom 'plot' visualization (x-axis: saturation, y-axis: brightness)")  
*A custom "plot" visualization" (x-axis: saturation, y-axis: brightness)*
